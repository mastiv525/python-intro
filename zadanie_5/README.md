# Web Scraping Lab

Projekt â€Web Scraping Labâ€ prezentuje praktyczne przykÅ‚ady pobierania i analizowania stron WWW za pomocÄ… Pythona.

## ğŸ“‚ Struktura katalogu `zadanie_5`

```
python-intro/
â””â”€â”€ zadanie_5/
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ example_requests.py  # pobieranie HTML (wÄ…tkowo, zapisywanie do plikÃ³w)
    â”‚   â””â”€â”€ example_bs4.py       # parsowanie nagÅ‚Ã³wkÃ³w, linkÃ³w, tabel, eksport danych
    â”œâ”€â”€ requirements.txt        # lista bibliotek: requests, beautifulsoup4, pandas, openpyxl
    â”œâ”€â”€ README.md               # ten plik: instrukcja uruchomienia i opisu projektu
    â””â”€â”€ raport.md               # raport z wykonania zadania
```

## âš™ï¸ Wymagania

* Python 3.7 lub nowszy
* ModuÅ‚y Python:

  * `requests`
  * `beautifulsoup4`
  * `pandas`
  * `openpyxl`

Zainstalujesz je poleceniem:

```bash
pip install -r requirements.txt
```

## ğŸš€ Uruchomienie

1. **Sklonuj repozytorium**

 

git clone [https://github.com/mastiv525/python-intro.git](https://github.com/mastiv525/python-intro.git)
cd python-intro/zadanie\_5

````
2. **(Opcjonalnie) UtwÃ³rz i aktywuj** Å›rodowisko wirtualne:
   ```bash
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
````

3. **Zainstaluj zaleÅ¼noÅ›ci**

   ```bash
   pip install -r requirements.txt
   ```



````
4. **Uruchom przykÅ‚ad pobierania HTML**
   ```bash
python examples/example_requests.py <URL1> <URL2> -t 4 -o prefix
````

5. **Uruchom parsowanie HTML**

   ```bash
   python examples/example\_bs4.py <URL>
   ```


6. **Wnioski**
```

## ğŸ“„ Opis skryptÃ³w
- **example_requests.py**
  - Pobiera wiele stron rÃ³wnolegle (ThreadPoolExecutor)
  - Mierzy czas pobrania i zapisuje kaÅ¼dy HTML do pliku `prefix_<url>.html`
  - UÅ¼ywa `argparse` (URL, --threads, --output-prefix) i `logging` dla czytelnych komunikatÃ³w

- **example_bs4.py**
  - Parsuje pobrany HTML (BeautifulSoup)
  - Wylicza liczbÄ™ nagÅ‚Ã³wkÃ³w H1â€“H3 i zapisuje je do `headings.csv`
  - Zbiera wszystkie linki `http` i zapisuje do `links.txt` i `links.csv`
  - Ekstraktuje tabele HTML (pandas.read_html) i zapisuje do arkusza `tables.xlsx`

## âš ï¸ Ograniczenia
- `requests` nie renderuje JavaScript â€” strony dynamiczne (SPA) mogÄ… byÄ‡ niekompletne
- Brak automatycznej obsÅ‚ugi `robots.txt` i nagÅ‚Ã³wkÃ³w przeglÄ…darki â€” moÅ¼e dojÅ›Ä‡ do blokad serwera
- Skrypty nie obsÅ‚ugujÄ… paginacji ani nawigacji przez wiele podstron
- Struktura HTML rÃ³Å¼ni siÄ™ w zaleÅ¼noÅ›ci od serwisu â€” nie wszystkie tabele i nagÅ‚Ã³wki zostanÄ… poprawnie zidentyfikowane

## ğŸ”® MoÅ¼liwe usprawnienia
- Integracja z Selenium lub Playwright dla stron wymagajÄ…cych JavaScript
- Dodanie cache lokalnego, by unikaÄ‡ wielokrotnych zapytaÅ„ do tych samych stron
- ObsÅ‚uga paginacji i automatyczne Å›ledzenie linkÃ³w wewnÄ™trznych
- Rozszerzenie nagÅ‚Ã³wkÃ³w HTTP i zarzÄ…dzanie sesjami (ciasteczka, tokeny)
- Automatyczne testy jednostkowe i integracyjne dla crawlera

---
*Autor: mastiv525*

```
